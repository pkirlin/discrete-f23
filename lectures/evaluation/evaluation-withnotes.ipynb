{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f3ca29e-a58c-4625-9ea9-fb3d263b3084",
   "metadata": {},
   "source": [
    "# Evaluating Machine Learning Models\n",
    "\n",
    "Once we have a machine learning model, how do we report how well it performs?  We need some way\n",
    "to measure this.\n",
    "\n",
    "## Evaluation metrics\n",
    "\n",
    "- There are various numerical ways to measure the performance of our machine learning models.\n",
    "\n",
    "- These generally correspond to our cost functions, $J(w)$, for whatever model we are using, though there\n",
    "are others.\n",
    "\n",
    "### Regression metrics\n",
    "\n",
    "- For regression problems (predicting a number), we can use **mean squared error** (which we use to train\n",
    "linear regression):\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "but this is not the only metric we can use.\n",
    "\n",
    "- Some people recommend the **mean absolute error**:\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{m} \\sum_{i=1}^m \\left\\vert \\hat{y}^{(i)} - y^{(i)} \\right\\vert$$\n",
    "\n",
    "One advantage MAE has is the quantity is more \"interpretable\" in that it has the same units as the target\n",
    "value $y$.  For example, if we are predicting housing prices in dollars, then the MAE tells us, on average, how\n",
    "far away our predictions are from the \"true\" price of a house.  On the other hand, MSE gives us a value \n",
    "in \"square dollars\" which is harder to interpret.\n",
    "\n",
    "- We can also take the square root of the mean squared error to get the **root-mean-square error**:\n",
    "\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}} = \\sqrt{ \\frac{1}{m} \\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2 }$$\n",
    "\n",
    "This metric also has the property that the units match the units of $y$, though some people still recommend\n",
    "MAE over RMSE.\n",
    "\n",
    "- There is one more common metric, known as **mean absolute percentage error**:\n",
    "\n",
    "$$\\text{MAE} = \\frac{1}{m} \\sum_{i=1}^m \\left\\vert \\dfrac{\\hat{y}^{(i)} - y^{(i)}}{y^{(i)}} \\right\\vert$$\n",
    "\n",
    "This metric has the advantage that it will always be between 0 and 1, and therefore can be \n",
    "interpreted as a percentage.\n",
    "\n",
    "<hr>\n",
    "\n",
    "For all of these metrics, lower numbers indicate better predictions.\n",
    "\n",
    "### Classification metrics\n",
    "\n",
    "For binary classification problems, we can use any of the metrics above with 0 and 1 for our target\n",
    "variable $y$.  For $\\hat{y}$, we can use 0 or 1 if our model directly predicts the class, or if the model\n",
    "predicts a probability between 0 and 1, we can use that as well.\n",
    "\n",
    "However, there are a number of more common metrics:\n",
    "\n",
    "- **Accuracy** is probably the simplest and most common.  It just measures the number of values\n",
    "predicted correctly divided by the total number of values.\n",
    "\n",
    "Accuracy, however, can be misleading when the number of data points in each of our classes\n",
    "is unbalanced.  Consider an example where we have 100 data points, but 95 of them are from the positive\n",
    "class and only 5 are from the negative class.  Assume we have a machine learning model that always \n",
    "predicts the positive class ($f(x) = 1$), no matter what $x$ is.  The accuracy of this model\n",
    "will be 95%, but obviously it will always miss predictions for the negative class, which probably isn't\n",
    "very useful.\n",
    "\n",
    "We have a few ways to make better metrics:\n",
    "\n",
    "Let's assume our classification problem is trying to predict whether someone has a disease or not.  \n",
    "We build a binary classifier for this and will predict either 1 (has the disease) or 0 (does not \n",
    "have the disease).  There are some standardized terms we can use:\n",
    "\n",
    "- A **true positive** is predicting 1 (has the disease) when the patient does have the disease.\n",
    "- A **true negative** is prediction 0 (does not have the disease) when the patient does not have the disease.\n",
    "- A **false positive** is predicting 1 when the patient does **not** have the disease.\n",
    "- A **false negative** is predicting 0 when the patient **does** have the disease.\n",
    "\n",
    "We will use TP, TN, FP, and FN as abbreviations for the total number of these occurrences in a data set.\n",
    "\n",
    "A few common metrics using these values are:\n",
    "\n",
    "- **True positive rate (TPR)**: $\\dfrac{TP}{TP+FN}$.  \n",
    "\n",
    "- **True negative rate (TNR)**: $\\dfrac{TN}{TN+FP}$. \n",
    "\n",
    "- **Balanced accuracy** = $\\dfrac{TPR + TNR}{2}$\n",
    "\n",
    "There are others that are useful in certain situations, such as precision, recall, sensitivity, specificity,\n",
    "F-score, etc.\n",
    "\n",
    "## Training sets and testing sets\n",
    "\n",
    "- Recall that in machine learning, we are primarily concerned with making models that can **generalize**\n",
    "to new data (data the model has not seen; data the model was not trained on).  Therefore, when we want\n",
    "to evaluate a model, we must do it on a different data set than the data the model was trained on. \n",
    "\n",
    "- Traditionally, if we have enough data, we will split all of the data we have available to us into\n",
    "a **training set** and a **testing set**.  The sizes of these sets can vary; you will see recommendations\n",
    "from using 2/3 of your data for training and the last 1/3 for testing, to using 80% for training and 20%\n",
    "for testing.\n",
    "\n",
    "- The easiest thing to do is train your model on the training set, and then test on the testing set.\n",
    "We will report metrics (from above, like accuracy or MSE), on both sets, but we are often more interested\n",
    "in the testing set evaluation.  However, performing evaluation on both sets can often reveal other \n",
    "interesting things (for example, if we get a small amount of errors on the training set, but a large number\n",
    "of errors on the testing set, this often indicates overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c7fb2-a3e0-495b-8fa3-8e5ac0363b06",
   "metadata": {},
   "source": [
    "## What if I don't have enough data?\n",
    "\n",
    "While large data sets are now common in machine learning, we don't always have enough data to split\n",
    "the data set into a large enough subset to train on, plus a large enough set to test on as well.  Or,\n",
    "even if we do have enough data, we know that training a model on **more** data almost always results in a\n",
    "better model, so it seems silly to let 1/3 or 20% of our data \"go to waste\" and not use it for training.\n",
    "\n",
    "A common solution to this is called **cross-validation**, and there are a few ways to do it:\n",
    "\n",
    "- **$k$-fold cross-validation**: This is a nice method if you have a pretty big data set but you don't feel like you can hold back 1/3 or 20% of it for testing.\n",
    "\n",
    "  We will partition the entire data set into $k$ equally-sized subsets.  Of the $k$ subsets, we choose one of them to be the testing set, and the other $k-1$ subsets combined will form the training set.  We train our model on the training set and test it on the testing set as normal.  Then, we repeat this process but **using a different subset of the original $k$ subsets for the testing set**.  We will usually then average all the evaluations from each of the $k$ individual training/testing cycles.\n",
    "  \n",
    "- **Leave $p$ out cross-validation**: This is even more extreme than $k$-fold cross validation, but\n",
    "is nice when your data set is very small.  \n",
    "\n",
    "  In this method, we remove $p$ data points from the data set and use only those $p$ points for testing. The rest of the data is used for training.  We repeat this process with every possible set of $p$ data points from the original data, and average all of the evaluations on the different testing sets.\n",
    "  \n",
    "  This method quickly becomes unwieldy for large values of $p$, but it is very commonly used with $p=1$, or **leave-one-out cross-validation (LOOCV)**.  Here, we cycle through our entire data set and use each individual data point in turn as the sole member of the testing set, training on all the rest of the data except that one point.  We average all the results.\n",
    "  \n",
    "<hr>\n",
    "\n",
    "<b>The key idea in all of these methods is that we never use the same data point in both the testing and training sets.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d50faea-9301-49c7-bc5b-81c72ec47685",
   "metadata": {},
   "source": [
    "## A complication: validation sets\n",
    "\n",
    "In many situations, we need a third data set, which is known as a **validation set**.  \n",
    "\n",
    "Imagine the following situation: you are training a model with linear regression, but you don't know what features to use.  (This might be because you have a large set of features that you need to cut down, or maybe you think you might need to create some new features through feature engineering.)  \n",
    "\n",
    "In this situation, you might need to create a bunch of different linear regression models with different combinations of features and evaluate how well they do.  If you simply use a single training set and a single testing set in this situation (and therefore evaluate all the different models using the same testing set), you are effectively now using the testing set for training.  This is because presumably you will choose the model from all the possible models you create that performs best on the testing data.  But that's exactly what we're trying to avoid --- having any overlap between the data we're using to create the model and the data we use to evaluate how well the model is doing.  (And here, \"creating\" the model also includes picking what features to use.)\n",
    "\n",
    "To handle this, we introduce a third data set: the **validation set**.  This is another subset of your data that you will use if you need to evaluate multiple models.  You use the validation set as \"testing data\" for choosing the best model, then you use the \"true\" testing set to evaluate the best model once you've found it (but then you can't go back and switch to a different model based on the evaluation on the testing set).\n",
    "\n",
    "The point of this is that the testing set is only used at the very end, once all the details of the model are fully specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f024e4-42c8-4e50-ac33-3cffe0582cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
