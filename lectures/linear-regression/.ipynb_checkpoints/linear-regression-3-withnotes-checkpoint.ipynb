{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78107bc-1140-459f-ba6d-0ef0f1b29095",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "In our original formulation of linear regression, we had a single feature (input variable).  Now we will \n",
    "expand to multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef16bf5c-1f91-47a9-962e-577c72e60110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft   price\n",
       "0  2104  399900\n",
       "1  1600  329900\n",
       "2  2400  369000\n",
       "3  1416  232000\n",
       "4  3000  539900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"housing.csv\")\n",
    "df[['sqft', 'price']].head()    # head() just gives you the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ec16eb-a6cd-48df-84f9-538c736c908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft  bedrooms   price\n",
       "0  2104         3  399900\n",
       "1  1600         3  329900\n",
       "2  2400         3  369000\n",
       "3  1416         2  232000\n",
       "4  3000         4  539900"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New data:\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efaaa6-a76d-4fe5-866b-fc83f5a1f19c",
   "metadata": {},
   "source": [
    "Previously, our single feature was denoted by $x$.  Now, we will denote each feature by \n",
    "subscripting $x$: $x_1$, $x_2$, $x_3$, etc., up to however many features we have.\n",
    "\n",
    "So above, we will call `sqft` $x_1$ and `bedrooms` $x_2$.  `price` is still our \"output feature\" --- the thing we're\n",
    "trying to predict, denoted by $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b3e65-fd09-4088-bf5d-45e3ba0ad321",
   "metadata": {},
   "source": [
    "In the real world, we might have addition features like number of floors, age of the house, etc.\n",
    "\n",
    "## Notation\n",
    "\n",
    "- We'll use $x_j$ to represent the $j$'th feature.\n",
    "\n",
    "- We'll use $n$ to represent the number of features.\n",
    "\n",
    "- $\\boldsymbol{x}^{(i)}$ will still represent the $i$'th training example, but now $\\boldsymbol{x}$ is a vector!\n",
    "\n",
    "- You will sometimes see $\\vec{x}$ instead.\n",
    "\n",
    "- We will usually write vectors as column vectors (this makes the math easier), though row vectors also exist.\n",
    "\n",
    "$$\\boldsymbol{x}^{(i)} = \\begin{bmatrix}\n",
    "           x_{1}^{(i)} \\\\\n",
    "           x_{2}^{(i)} \\\\\n",
    "           \\vdots \\\\\n",
    "           x_{n}^{(i)}\n",
    "         \\end{bmatrix}$$\n",
    "         \n",
    "**Old Model**: $f_{w, b}(x) = wx+b$\n",
    "\n",
    "**New Model**: $f_{\\boldsymbol{w}, b}(\\boldsymbol{x}) = w_1x_1 + w_2x_2 + \\cdots + b$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "**New Model**: $f_{\\boldsymbol{w}, b}(\\boldsymbol{x}) = \\displaystyle \\sum_{j=1}^n w_jx_j + b$\n",
    "\n",
    "**Example**: If our model for housing prices is $f(x) = 0.1x_1 + 4x_2 + 80$, then you should think of this as \"the base price of a house is 80 thousand dollars, and for every square foot ($x_1$) we add to the house, the price goes up by 0.1 thousand dollars (\\$100), and for every bedroom we add to the house, the price goes up by 4 thousand dollars.\n",
    "\n",
    "## Vectors\n",
    "\n",
    "- We normally will also put our values for $w$ into a matrix:\n",
    "\n",
    "$$\\boldsymbol{w} = \\begin{bmatrix}\n",
    "           w_{1} \\\\\n",
    "           w_{2} \\\\\n",
    "           \\vdots \\\\\n",
    "           w_{n}\n",
    "         \\end{bmatrix}$$\n",
    "         \n",
    "- Because there are $n$ features, there are $n$ elements in this vector, one per feature.\n",
    "\n",
    "- And now we can write our formula for $f$ much more simply:\n",
    "\n",
    "- $f_{\\boldsymbol{w}, b}(\\boldsymbol{x}) = \\boldsymbol{w} \\cdot \\boldsymbol{x} + b$ where the dot is the **dot product**.\n",
    "\n",
    "- You will sometimes also see this written as \n",
    "\n",
    "- $f_{\\boldsymbol{w}, b}(\\boldsymbol{x}) = \\boldsymbol{w}^T  \\boldsymbol{x} + b$ where now we are doing **matrix multiplication**.\n",
    "\n",
    "- This process is called vectorization.  We do this for a few reasons.  \n",
    "  - Mathematical simplicity: makes our formulas look nicer, and it's easier to understand as long as you understand the symbols.\n",
    "  - Computational simplicity: lets us write code using matrix libraries (like numpy).\n",
    "  - Computational speed: many of these libraries are optimized for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5a9c8-626f-470f-a6d3-d6143940b142",
   "metadata": {},
   "source": [
    "- Example:\n",
    "\n",
    "    ```\n",
    "    f = w[0] * x[0] + w[1] * x[1] + b  # Not great\n",
    "    ```\n",
    "    \n",
    "    <P><hr>\n",
    "    \n",
    "    ```\n",
    "    f = 0\n",
    "    for j in range(0, n):\n",
    "        f += w[j] * x[j]\n",
    "    f += b                            # better\n",
    "    ```\n",
    "    \n",
    "    <P><hr>\n",
    "    \n",
    "    ```\n",
    "    f = np.dot(w, x) + b              # best\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c665ef-6193-45b2-b087-84b011911099",
   "metadata": {},
   "source": [
    "- Vectorized code often can use parallel hardware like multiple CPUs or GPUs.  Often the libraries (like NumPy) are implemented in such a way that even if we write the function calls in Python (like `np.dot(...)`), the libraries are implemented in (compiled) C code behind the scenes.\n",
    "\n",
    "- We will see an improvement later to make it even faster!  (Get rid of that $b$ at the end).\n",
    "\n",
    "- Vectorizing our code is the difference between time to train our model taking hours vs minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d74b321-0a2e-45b0-bddf-1464ff0ee554",
   "metadata": {},
   "source": [
    "## Gradient descent for multiple linear regression\n",
    "\n",
    "**Old cost function:** $J(w,b)$\n",
    "\n",
    "**New cost function:** $J(w_1, w_2, \\ldots, b)$ or using vector notation \n",
    "$J(\\boldsymbol{w}, b)$\n",
    "\n",
    "The gradient descent update equations do not change much:\n",
    "\n",
    "$$w_j = w - \\alpha \\cdot \\frac{\\partial}{\\partial w} J(\\boldsymbol{w}, b)$$\n",
    "\n",
    "$$b = b - \\alpha \\cdot \\frac{\\partial}{\\partial b} J(\\boldsymbol{w}, b)$$\n",
    "\n",
    "The derivative calculation itself doesn't change much either, we just\n",
    "basically add a subscript to the $w$'s, so gradient descent becomes:\n",
    "\n",
    "**repeat until convergence** {\n",
    "$$w_1 = w_1 - \\alpha  \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w},b}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x_1^{(i)}$$\n",
    "\n",
    "$$w_2 = w_2 - \\alpha  \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w},b}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x_2^{(i)}$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$b = b - \\alpha  \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w},b}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  $$\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be50ce02-d17c-48fa-8d49-1b7399fee6c9",
   "metadata": {},
   "source": [
    "## Real world considerations\n",
    "\n",
    "### Feature scaling\n",
    "\n",
    "- Gradient descent can have problems converging or fail to converge when we use features will very different ranges.\n",
    "\n",
    "- Consider our two features $x_1$ and $x_2$, square feet and number of bedrooms.  One ranges from about 300-2000, and the other ranges from about 0-5.\n",
    "\n",
    "- What will naturally happen is that because these features have different magnitudes, gradient descent will probably eventually find a small weight for $w_1$ and a larger weight for $w_2$ (though not necessarily).\n",
    "\n",
    "- But if we start with random initial weights, or we make them all 1 at the beginning,\n",
    "gradient descent may have a hard time figuring out to adjust the weights, because it may end up \"bouncing around\" the contour plot of the cost function $J$ before\n",
    "getting on track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7f4583-4378-4cec-9b26-04df3eac89cb",
   "metadata": {},
   "source": [
    "### So what do we do about this?\n",
    "\n",
    "A few solutions, ranging from simple to complex (actually they're all pretty simple!):\n",
    "\n",
    "- Divide each feature by its maximum.  So if $x_1$ (square feet) ranges from 300-2000, we would not feed $x_1$ directly into our linear regression computations; instead we would use $x_1/2000$.  This effectively changes the maximum of the feature to be 1.\n",
    "\n",
    "- Mean normalization: Replace each feature by the calculation: $$\\frac{x_j-\\mu_j}{\\max(x_j) - \\min(x_j)}$$ where $\\mu_j$ is the mean value of $x_j$.\n",
    "\n",
    "  This changes each feature to range between -1 and 1.\n",
    "  \n",
    "- Z-score normalization: Replace each feature by the calculation: $$\\frac{x_j-\\mu_j}{\\sigma_j}$$ where $\\mu_j$ is the mean value of $x_j$ and $\\sigma_j$ is\n",
    "the standard deviation of $x_j$.\n",
    "\n",
    "  This changes each feature to range between \"small-ish\" values, but unlike mean normalization which will always compress the entire range between -1 and 1, Z-score normalization takes into account how \"spread out\" the values are and what their distribution looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808534e-d531-449e-ace5-6430705b50fe",
   "metadata": {},
   "source": [
    "### In general\n",
    "\n",
    "- Aim for each feature to be \"roughly\" between -1 and 1, within an order of magnitude.\n",
    "\n",
    "- All of these ranges are fine (to co-exist): \n",
    "  - -1 to -1\n",
    "  - -3 to 3\n",
    "  - -0.3 to 0.3\n",
    "  - 0 to 3 \n",
    "  - -2 to 0.5\n",
    "  \n",
    "- But if you add something like -100 to 100 to the features above, it will \"overpower\" the existing features and might throw off gradient descent.\n",
    "\n",
    "- Similarly, adding a feature like -0.001 to 0.001 might have the same effect.\n",
    "\n",
    "- Interestingly, adding a feature with a large/small magnitude but a small range might also cause problems.  For instance, including a feature with the range 97-106 (e.g., body temperature) would be problematic and should be rescaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031a9ec-bb52-477f-9029-0b869d212de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking gradient descent for convergence\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
