{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69b3ed-5f7e-44a3-9ed9-1520ecc416d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data specifically cubic data, y= x^{3}- 3x + 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "x_data = np.arange(-3, 3, .1)\n",
    "y_numeric = x_data ** 3 - 3 * x_data + 2\n",
    "y_categorical = (y_numeric >= 2).astype(int)\n",
    "\n",
    "# add noise\n",
    "noise = np.array([ 1.20109968,  0.5143968 , -0.75046677,  0.61609437, -1.42564964,\n",
    "       -0.24278509, -0.06682367, -0.07221022, -1.25231036,  0.10505861,\n",
    "        0.83387709, -1.49745355,  0.39227633, -1.45015043,  1.02000935,\n",
    "       -0.92474454,  1.46654345, -0.22729801, -2.68092581,  0.19022616,\n",
    "        1.460804  ,  0.05054745, -0.32031863, -0.20126538,  1.45846302,\n",
    "       -0.18968677,  1.1490469 , -1.13536056, -0.37933621,  1.09194987,\n",
    "       -0.62442628,  1.23550369,  1.34559668, -0.40558424,  1.20431376,\n",
    "        1.44755194, -0.24809615, -0.38604721,  1.07668887,  0.80339283,\n",
    "       -1.03546213, -1.29254652,  2.92751962,  0.17439257, -1.40184363,\n",
    "       -1.31655769, -0.32464697, -0.01413023, -1.27810733,  1.02339441,\n",
    "       -1.02522028, -0.42611774, -0.37729729,  1.1996452 ,  0.79166404,\n",
    "        1.20745722, -0.37161452, -0.19916778,  1.20829573,  1.28583724])\n",
    "\n",
    "y_numeric_noisy = y_numeric + noise\n",
    "y_categorical_noisy = (y_numeric_noisy >= 2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe39454-89a7-4eb9-9916-9778915f963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original data\n",
    "\n",
    "plt.plot(x_data, y_numeric, -3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3ac06-4189-4008-8853-c64ef8f9d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot noisy data\n",
    "\n",
    "plt.plot(x_data, y_numeric_noisy, -3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d08332-aca0-4a36-8d70-f95855644acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = np.array([(i, j) for i, j in zip(x_data, y_numeric) if j >= 2])\n",
    "negatives = np.array([(i, j) for i, j in zip(x_data, y_numeric) if j < 2])\n",
    "plt.scatter(positives[:,0], positives[:,1])\n",
    "plt.scatter(negatives[:,0], negatives[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbb34b-3335-42aa-926b-1bd7ecfd9000",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = np.array([(i, j) for i, j in zip(x_data, y_numeric_noisy) if j >= 2])\n",
    "negatives = np.array([(i, j) for i, j in zip(x_data, y_numeric_noisy) if j < 2])\n",
    "plt.scatter(positives[:,0], positives[:,1])\n",
    "plt.scatter(negatives[:,0], negatives[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e3947-ad2c-4142-8abb-8cd079908437",
   "metadata": {},
   "source": [
    "# Neural Network 1: Regression, no hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22610dfa-d192-4c90-a1f3-e3c5f08d7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network 1, no hidden layer, regression (should duplicate linear regression)\n",
    "\n",
    "# Activation function is identity function (not sigmoid)\n",
    "# Loss function is squared error\n",
    "\n",
    "X_train = np.array(x_data).reshape((-1, 1))\n",
    "X_train = np.hstack([ np.ones((len(x_data), 1)), X_train ])\n",
    "y_train = y_numeric_noisy\n",
    "\n",
    "W = np.array([[.1, .2]])\n",
    "\n",
    "def compute_z_vector(W, input_vector):\n",
    "    return W @ input_vector\n",
    "\n",
    "def compute_activation_vector(z_vector):\n",
    "    return z_vector # identity function\n",
    "\n",
    "def compute_loss(a, y):\n",
    "    return .5 * (a - y) ** 2\n",
    "\n",
    "def compute_cost(X_data, y_data, W):\n",
    "    m = X_data.shape[0] # number of data points\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat = make_prediction(X_data[i], W)\n",
    "        y = y_data[i]\n",
    "        cost += compute_loss(y_hat, y)\n",
    "    total_cost = (1 / (2 * m)) * cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W, x, **kwargs):\n",
    "    # process kwargs\n",
    "    verbose = kwargs['verbose'] if 'verbose' in kwargs else False\n",
    "    output_only = kwargs['output_only'] if 'output_only' in kwargs else False\n",
    "    \n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # set a0\n",
    "    a0 = x\n",
    "    if verbose: print(\"Input to this layer is\", a0)\n",
    "\n",
    "    # get W matrix\n",
    "    if verbose: print(\"Using W of shape\", W.shape)\n",
    "\n",
    "    # compute z\n",
    "    z1 = compute_z_vector(W, a0)\n",
    "    if verbose: print(\"z is\", z1)\n",
    "\n",
    "    # compute activation\n",
    "    a1 = compute_activation_vector(z1)\n",
    "    if verbose: print(\"activation is\", a1)\n",
    "    \n",
    "    if output_only:\n",
    "        return a1\n",
    "    else:\n",
    "        return z1, a1\n",
    "    \n",
    "def make_prediction(x, W):\n",
    "    return forward_prop(W, x, output_only=True)\n",
    "\n",
    "def deriv_loss(a, y):\n",
    "    return (a-y)\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return 1\n",
    "\n",
    "def backward_prop(W, x, y, z1, a1):\n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # compute dL/dw0 \n",
    "    dL_da = deriv_loss(a1, y)\n",
    "    da_dz = deriv_activation(z1)\n",
    "    dz_dw0 = x[0]\n",
    "    # multiply\n",
    "    dL_dw0 = dL_da * da_dz * dz_dw0\n",
    "    \n",
    "    # compute dL/dw0 (first two pieces already done)\n",
    "    dz_dw1 = x[1]\n",
    "    # multiply\n",
    "    dL_dw1 = dL_da * da_dz * dz_dw1\n",
    "    \n",
    "    return dL_dw0, dL_dw1\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1746561-d059-45fa-b288-ce2533bfae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, a1 = forward_prop(W, X_train[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db18e597-fa5b-4c53-9137-ee01d51c6c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(X_train, y_train, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c666f609-63f7-47ea-b462-19ee1691edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_prop(W, X_train[0], y_train[0], z1, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674bd5d5-1700-4ce7-80f8-e1a8d2f2d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent:\n",
    "\n",
    "W = np.array([[.1, .2]])\n",
    "ALPHA = .003\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "print(\"x train is\", X_train)\n",
    "print(\"y train is\", y_train)\n",
    "\n",
    "for ctr in range(0, 20):\n",
    "    #print(\"Iteration: \", ctr)\n",
    "    #print(\"W =\", W)\n",
    "    #print(\"Cost is\", compute_cost(X_train, y_train, W))\n",
    "    \n",
    "    for i in range(X_train.shape[0]):   # m\n",
    "        z1, a1 = forward_prop(W, X_train[i])\n",
    "        dL_dw0, dL_dw1 = backward_prop(W, X_train[i], y_train[i], z1, a1)\n",
    "        #print(\"Gradients\", dL_dw0, dL_dw1)\n",
    "        W[0][0] -= ALPHA * dL_dw0\n",
    "        W[0][1] -= ALPHA * dL_dw1\n",
    "        J_sequence.append(compute_cost(X_train, y_train, W))\n",
    "        \n",
    "    # \"batch\" gradient descent:\n",
    "    #D = [0,0]\n",
    "    #for i in range(X_train.shape[0]):   # m\n",
    "    #    z1, a1 = forward_prop(W, X_train[i])\n",
    "    #    dL_dw0, dL_dw1 = backward_prop(W, X_train[i], y_train[i], z1, a1)\n",
    "    #    #print(\"Gradients\", dL_dw0, dL_dw1)\n",
    "    #    #W[0][0] -= ALPHA * dL_dw0\n",
    "    #    #W[0][1] -= ALPHA * dL_dw1\n",
    "    #    D[0] += dL_dw0\n",
    "    #   D[1] += dL_dw1   \n",
    "    #W[0][0] -= ALPHA * D[0]\n",
    "    #W[0][1] -= ALPHA * D[1]\n",
    "                              \n",
    "    J_sequence.append(compute_cost(X_train, y_train, W))\n",
    "    \n",
    "print(\"Final W:\", W)\n",
    "print(\"Final cost:\", compute_cost(X_train, y_train, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c900d3-10aa-4efb-bb8c-54bd1962b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bec448-3a04-4882-a730-bd01904ec746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate original plot\n",
    "\n",
    "y_predicted = [make_prediction([1, x], W) for x in x_data]\n",
    "plt.plot(x_data, y_numeric_noisy, -3, 3)\n",
    "plt.plot(x_data, y_predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecb861-b930-40ba-b44f-6ed040a753c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify this matches linear regression\n",
    "\n",
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LinearRegression(fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# model.coef_ contains the w vector that this linear regression model was able to find\n",
    "\n",
    "print(\"w found through scikit-learn:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc2762-d5ac-4d07-bd19-e4b33aa8853e",
   "metadata": {},
   "source": [
    "# Neural network 2: Classification, no hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8f3b5-9950-49a9-8fe8-c73398f73919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network 2: Classification, no hidden layer (should duplicate logistic regression)\n",
    "\n",
    "# Activation function is sigmoid\n",
    "# Loss function is cross entropy\n",
    "\n",
    "X_train = np.array(x_data).reshape((-1, 1))\n",
    "X_train = np.hstack([ np.ones((len(x_data), 1)), X_train ])\n",
    "y_train = y_categorical_noisy\n",
    "\n",
    "W = np.array([[.1, .2]])\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n",
    "def compute_z_vector(W, input_vector):\n",
    "    return W @ input_vector\n",
    "\n",
    "def compute_activation_vector(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def compute_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def compute_cost(X_data, y_data, W):\n",
    "    m = X_data.shape[0] # number of data points\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat = make_prediction(X_data[i], W)\n",
    "        y = y_data[i]\n",
    "        cost += compute_loss(y_hat, y)\n",
    "    total_cost = (1 / m) * cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W, x, **kwargs):\n",
    "    # process kwargs\n",
    "    verbose = kwargs['verbose'] if 'verbose' in kwargs else False\n",
    "    output_only = kwargs['output_only'] if 'output_only' in kwargs else False\n",
    "    \n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # set a0\n",
    "    a0 = x\n",
    "    if verbose: print(\"Input to this layer is\", a0)\n",
    "\n",
    "    # get W matrix\n",
    "    if verbose: print(\"Using W of shape\", W.shape)\n",
    "\n",
    "    # compute z\n",
    "    z1 = compute_z_vector(W, a0)\n",
    "    if verbose: print(\"z is\", z1)\n",
    "\n",
    "    # compute activation\n",
    "    a1 = compute_activation_vector(z1)\n",
    "    if verbose: print(\"activation is\", a1)\n",
    "    \n",
    "    if output_only:\n",
    "        return a1\n",
    "    else:\n",
    "        return z1, a1\n",
    "    \n",
    "def make_prediction(x, W):\n",
    "    return forward_prop(W, x, output_only=True)\n",
    "\n",
    "def deriv_loss(a, y):\n",
    "    return (a-y)/(a*(1-a))\n",
    "\n",
    "def deriv_activation(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "def backward_prop(W, x, y, z1, a1):\n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # compute dL/dw0 \n",
    "    dL_da = deriv_loss(a1, y)\n",
    "    da_dz = deriv_activation(z1)\n",
    "    dz_dw0 = x[0]\n",
    "    # multiply\n",
    "    dL_dw0 = dL_da * da_dz * dz_dw0\n",
    "    \n",
    "    # compute dL/dw0 (first two pieces already done)\n",
    "    dz_dw1 = x[1]\n",
    "    # multiply\n",
    "    dL_dw1 = dL_da * da_dz * dz_dw1\n",
    "    \n",
    "    return dL_dw0, dL_dw1\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccc08d-c435-423f-b704-9cb50c092961",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, a1 = forward_prop(W, X_train[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eebf54e-0748-4be3-a650-feab7d5b3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(X_train, y_train, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424881b3-6030-4844-b126-b510cf5274c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_prop(W, X_train[0], y_train[0], z1, a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df76e3e-fb77-47f1-9fce-11d76a5d90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent:\n",
    "\n",
    "W = np.array([[.1, .2]])\n",
    "ALPHA = .001\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "print(\"x train is\", X_train)\n",
    "print(\"y train is\", y_train)\n",
    "\n",
    "for ctr in range(0, 200):\n",
    "    #print(\"Iteration: \", ctr)\n",
    "    #print(\"W =\", W)\n",
    "    #print(\"Cost is\", compute_cost(X_train, y_train, W))\n",
    "    \n",
    "    for i in range(X_train.shape[0]):   # m\n",
    "        z1, a1 = forward_prop(W, X_train[i])\n",
    "        dL_dw0, dL_dw1 = backward_prop(W, X_train[i], y_train[i], z1, a1)\n",
    "        #print(\"Gradients\", dL_dw0, dL_dw1)\n",
    "        W[0][0] -= ALPHA * dL_dw0\n",
    "        W[0][1] -= ALPHA * dL_dw1\n",
    "                              \n",
    "    J_sequence.append(compute_cost(X_train, y_train, W))\n",
    "    \n",
    "print(\"Final W:\", W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0e691-8544-40c0-bd66-bc0226328d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf65724-f599-47d3-ac58-4b48b51d6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and install scikit-learn if not already done:\n",
    "%pip install scikit-learn\n",
    "\n",
    "# Import the logistic regression functionality from scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model and train it on our training data:\n",
    "model = LogisticRegression(random_state=0, penalty=None, fit_intercept=False).fit(X_train, y_train)\n",
    "\n",
    "# If the line above gives an error about penalty=None, try switching that part to penalty='none' instead.\n",
    "\n",
    "# model.coef_ contains the w vector that this logistic regression model was able to find.\n",
    "\n",
    "print(\"w found through scikit-learn:\", model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e3af9-1d6b-41b4-8d71-7f4cd1bb9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate original plot\n",
    "\n",
    "positives = np.array([(i, j) for i, j in zip(x_data, y_numeric_noisy) if j >= 2])\n",
    "negatives = np.array([(i, j) for i, j in zip(x_data, y_numeric_noisy) if j < 2])\n",
    "plt.scatter(positives[:,0], positives[:,1])\n",
    "plt.scatter(negatives[:,0], negatives[:,1])\n",
    "\n",
    "y_predicted = [make_prediction([1, x], W) for x in x_data]\n",
    "positives = np.array([(i, j[0]) for i, j in zip(x_data, y_predicted) if j >= .5])\n",
    "negatives = np.array([(i, j[0]) for i, j in zip(x_data, y_predicted) if j < .5])\n",
    "plt.scatter(positives[:,0], positives[:,1])\n",
    "plt.scatter(negatives[:,0], negatives[:,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28b73c-b2bb-47a5-ac8a-48e541f27e35",
   "metadata": {},
   "source": [
    "# Neural Network 3, 1 hidden layer with 2 neurons, regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baa8e48-eadd-414e-a496-2482bbff0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network 3, 1 hidden layer, regression\n",
    "\n",
    "# Activation function is sigmoid for layer 1, but identity for layer 2\n",
    "# Loss function is squared error\n",
    "\n",
    "X_train = np.array(x_data).reshape((-1, 1))\n",
    "X_train = np.hstack([ np.ones((len(x_data), 1)), X_train ])\n",
    "y_train = y_numeric_noisy\n",
    "\n",
    "W1 = np.array([[.1, .2], [.3, .4]])\n",
    "W2 = np.array([[.5, .6, .7]])\n",
    "            \n",
    "def compute_z_vector(W, input_vector):\n",
    "    return W @ input_vector\n",
    "\n",
    "def compute_activation_vector(z_vector):\n",
    "    return sigmoid(z_vector)\n",
    "\n",
    "def compute_activation_vector_final(z_vector):\n",
    "    return z_vector # identity function\n",
    "\n",
    "def compute_loss(a, y):\n",
    "    return .5 * (a - y) ** 2\n",
    "\n",
    "def compute_cost(X_data, y_data, W1, W2):\n",
    "    m = X_data.shape[0] # number of data points\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat = make_prediction(X_data[i], W1, W2)\n",
    "        y = y_data[i]\n",
    "        cost += compute_loss(y_hat, y)\n",
    "    total_cost = (1 / (2 * m)) * cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W1, W2, x, **kwargs):\n",
    "    # process kwargs\n",
    "    verbose = kwargs['verbose'] if 'verbose' in kwargs else False\n",
    "    output_only = kwargs['output_only'] if 'output_only' in kwargs else False\n",
    "    \n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # set input1\n",
    "    in1 = x\n",
    "    if verbose: print(\"Input to layer 1 is\", in1)\n",
    "\n",
    "    # get W1 matrix\n",
    "    if verbose: print(\"Using W1 of shape\", W1.shape, W1, in1)\n",
    "\n",
    "    # compute z\n",
    "    z1 = compute_z_vector(W1, in1)\n",
    "    if verbose: print(\"z1 is\", z1)\n",
    "\n",
    "    # compute activation\n",
    "    a1 = sigmoid(z1)\n",
    "    if verbose: print(\"a1 is\", a1)\n",
    "    \n",
    "    # set input2\n",
    "    in2 = augment_vector(a1)\n",
    "    if verbose: print(\"Input to layer 2 is\", in2)\n",
    "\n",
    "    # get W2 matrix\n",
    "    if verbose: print(\"Using W2 of shape\", W2.shape)\n",
    "\n",
    "    # compute z\n",
    "    z2 = compute_z_vector(W2, in2)\n",
    "    if verbose: print(\"z2 is\", z2)\n",
    "\n",
    "    # compute activation\n",
    "    a2 = z2  # identity\n",
    "    if verbose: print(\"a2 is\", a2)\n",
    "    \n",
    "    if output_only:\n",
    "        return a2\n",
    "    else:\n",
    "        return z1, a1, in2, z2, a2\n",
    "    \n",
    "def make_prediction(x, W1, W2):\n",
    "    return forward_prop(W1, W2, x, output_only=True)\n",
    "\n",
    "def deriv_loss(a, y):\n",
    "    return (a-y)\n",
    "\n",
    "def deriv_sigmoid(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "def deriv_identity(z):\n",
    "    return 1\n",
    "\n",
    "def backward_prop(W1, W2, x, y, z1, a1, in2, z2, a2):\n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # Calculations for layer 2:\n",
    "    \n",
    "    # compute dL/dw[2]00 \n",
    "    dL_da_2_0 = deriv_loss(a2[0], y)\n",
    "    da_2_0_dz_2_0 = deriv_identity(z2[0])\n",
    "    dz_2_0_dw_2_00 = in2[0]\n",
    "    # multiply\n",
    "    dL_dw_2_00 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_dw_2_00\n",
    "    \n",
    "    # compute dL/dw[2]01  [first two terms are the same as above]\n",
    "    dz_2_0_dw_2_01 = in2[1]\n",
    "    # multiply\n",
    "    dL_dw_2_01 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_dw_2_01\n",
    "    \n",
    "    # compute dL/dw[2]02  [first two terms are the same as above]\n",
    "    dz_2_0_dw_2_02 = in2[2]\n",
    "    # multiply\n",
    "    dL_dw_2_02 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_dw_2_02\n",
    "    \n",
    "    ## Calcs for layer 1:\n",
    "    \n",
    "    # compute dL/dw[1]00  [first two terms are the same as in layer 2]\n",
    "    dz_2_0_da_1_0 = W2[0][1]\n",
    "    da_1_0_dz_1_0 = deriv_sigmoid(z1[0])\n",
    "    dz_1_0_dw_1_00 = x[0]\n",
    "    # multiply\n",
    "    dL_dw_1_00 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_0 * da_1_0_dz_1_0 * dz_1_0_dw_1_00\n",
    "    \n",
    "    # compute dL/dw[1]01  [first four terms are the same as above]\n",
    "    dz_1_0_dw_1_01 = x[1]\n",
    "    # multiply\n",
    "    dL_dw_1_01 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_0 * da_1_0_dz_1_0 * dz_1_0_dw_1_01\n",
    "    \n",
    "    # compute dL/dw[1]10  [first two terms are the same as layer 2]\n",
    "    dz_2_0_da_1_1 = W2[0][2]\n",
    "    da_1_1_dz_1_1 = deriv_sigmoid(z1[1])\n",
    "    dz_1_1_dw_1_10 = x[0]\n",
    "    # multiply\n",
    "    dL_dw_1_10 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_1 * da_1_1_dz_1_1 * dz_1_1_dw_1_10\n",
    "    \n",
    "    # compute dL/dw[1]11  [first four terms are the same as above]\n",
    "    dz_1_1_dw_1_11 = x[1]\n",
    "    # multiply\n",
    "    dL_dw_1_11 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_1 * da_1_1_dz_1_1 * dz_1_1_dw_1_11\n",
    "    \n",
    "    # return updates to W1 and W2\n",
    "    \n",
    "    delta_W1 = np.array([[dL_dw_1_00, dL_dw_1_01], [dL_dw_1_10, dL_dw_1_11]])\n",
    "    delta_W2 = np.array([[dL_dw_2_00, dL_dw_2_01, dL_dw_2_02]])\n",
    "    return delta_W1, delta_W2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8179e64-a316-46e9-8b29-7ea049346651",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, a1, in2, z2, a2 = forward_prop(W1, W2, X_train[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a209fe-9c42-43cd-9450-b595f5f6fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cost(X_train, y_train, W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e998ca3-8a0e-4644-b533-418c0a079759",
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_prop(W1, W2, X_train[0], y_train[0], z1, a1, in2, z2, a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00611a3f-3ee1-497d-867e-65ea3ff16491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent:\n",
    "\n",
    "W1 = np.array([[.1, .2], [.3, .4]])\n",
    "W2 = np.array([[.5, .6, .7]])\n",
    "ALPHA = .01\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "print(\"x train is\", X_train)\n",
    "print(\"y train is\", y_train)\n",
    "\n",
    "for ctr in range(0, 500):\n",
    "    #print(\"Iteration: \", ctr)\n",
    "    #print(\"W =\", W1, W2)\n",
    "    print(\"Cost is\", compute_cost(X_train, y_train, W1, W2))\n",
    "    \n",
    "    for i in range(X_train.shape[0]):   # m\n",
    "        z1, a1, in2, z2, a2 = forward_prop(W1, W2, X_train[i])\n",
    "        delta_W1, delta_W2 = backward_prop(W1, W2, X_train[i], y_train[i], z1, a1, in2, z2, a2)\n",
    "        #print(\"Gradients\", delta_W1, delta_W2)\n",
    "        W1 -= ALPHA * delta_W1\n",
    "        W2 -= ALPHA * delta_W2\n",
    "        #J_sequence.append(compute_cost(X_train, y_train, W1, W2))\n",
    "        \n",
    "    #\"batch\" gradient descent:\n",
    "    #D = [np.full_like(W1, 0),np.full_like(W2, 0)]\n",
    "    #for i in range(X_train.shape[0]):   # m\n",
    "    #    z1, a1, in2, z2, a2 = forward_prop(W1, W2, X_train[i])\n",
    "    #    delta_W1, delta_W2 = backward_prop(W1, W2, X_train[i], y_train[i], z1, a1, in2, z2, a2)\n",
    "    #    #print(\"Gradients\", delta_W1, delta_W2)\n",
    "    #    #W[0][0] -= ALPHA * dL_dw0\n",
    "    #    #W[0][1] -= ALPHA * dL_dw1\n",
    "    #    D[0] += delta_W1\n",
    "    #    D[1] += delta_W2   \n",
    "    #W1 -= ALPHA * D[0]/X_train.shape[0]\n",
    "    #W2 -= ALPHA * D[1]/X_train.shape[0]\n",
    "                              \n",
    "    J_sequence.append(compute_cost(X_train, y_train, W1, W2))\n",
    "    \n",
    "print(\"Final W1, W2:\", W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa4b7c6-36a8-4d66-ae5a-067f1d06dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f97f13-5495-4e7a-b246-98f40da8d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate original plot\n",
    "print(W1, W2)\n",
    "y_predicted = [make_prediction([1, x], W1, W2) for x in x_data]\n",
    "plt.plot(x_data, y_numeric_noisy, -3, 3)\n",
    "plt.plot(x_data, y_predicted)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466051ef-4338-49db-b52f-aed405c6432f",
   "metadata": {},
   "source": [
    "# Neural Network 4, 1 hidden layer with 2 neurons, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063a3c81-aaf7-4d11-85a6-f7ff07a44d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network 4, 1 hidden layer, classification\n",
    "\n",
    "# Activation function is sigmoid for layer 1 and 2\n",
    "# Loss function is cross entropy\n",
    "\n",
    "X_train = np.array(x_data).reshape((-1, 1))\n",
    "X_train = np.hstack([ np.ones((len(x_data), 1)), X_train ])\n",
    "y_train = y_categorical_noisy\n",
    "\n",
    "W1 = np.array([[.1, .2], [.3, .4]])\n",
    "W2 = np.array([[.5, .6, .7]])\n",
    "            \n",
    "def compute_z_vector(W, input_vector):\n",
    "    return W @ input_vector\n",
    "\n",
    "def compute_loss(a, y):\n",
    "    if y == 0:\n",
    "        return -np.log(1-a)\n",
    "    else:\n",
    "        return -np.log(a)\n",
    "\n",
    "def compute_cost(X_data, y_data, W1, W2):\n",
    "    m = X_data.shape[0] # number of data points\n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        y_hat = make_prediction(X_data[i], W1, W2)\n",
    "        y = y_data[i]\n",
    "        cost += compute_loss(y_hat, y)\n",
    "    total_cost = (1 / m) * cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "def augment_vector(v):\n",
    "    return np.insert(v, 0, 1)\n",
    "\n",
    "def forward_prop(W1, W2, x, **kwargs):\n",
    "    # process kwargs\n",
    "    verbose = kwargs['verbose'] if 'verbose' in kwargs else False\n",
    "    output_only = kwargs['output_only'] if 'output_only' in kwargs else False\n",
    "    \n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # set input1\n",
    "    in1 = x\n",
    "    if verbose: print(\"Input to layer 1 is\", in1)\n",
    "\n",
    "    # get W1 matrix\n",
    "    if verbose: print(\"Using W1 of shape\", W1.shape, W1, in1)\n",
    "\n",
    "    # compute z\n",
    "    z1 = compute_z_vector(W1, in1)\n",
    "    if verbose: print(\"z1 is\", z1)\n",
    "\n",
    "    # compute activation\n",
    "    a1 = sigmoid(z1)\n",
    "    if verbose: print(\"a1 is\", a1)\n",
    "    \n",
    "    # set input2\n",
    "    in2 = augment_vector(a1)\n",
    "    if verbose: print(\"Input to layer 2 is\", in2)\n",
    "\n",
    "    # get W2 matrix\n",
    "    if verbose: print(\"Using W2 of shape\", W2.shape)\n",
    "\n",
    "    # compute z\n",
    "    z2 = compute_z_vector(W2, in2)\n",
    "    if verbose: print(\"z2 is\", z2)\n",
    "\n",
    "    # compute activation\n",
    "    a2 = sigmoid(z2)\n",
    "    if verbose: print(\"a2 is\", a2)\n",
    "    \n",
    "    if output_only:\n",
    "        return a2\n",
    "    else:\n",
    "        return z1, a1, in2, z2, a2\n",
    "    \n",
    "def make_prediction(x, W1, W2):\n",
    "    return forward_prop(W1, W2, x, output_only=True)\n",
    "\n",
    "def deriv_loss(a, y):\n",
    "    return (a-y)/(a*(1-a))\n",
    "\n",
    "def deriv_sigmoid(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "def deriv_identity(z):\n",
    "    return 1\n",
    "\n",
    "def backward_prop(W1, W2, x, y, z1, a1, in2, z2, a2):\n",
    "    # we assume x has the 1 in front.\n",
    "    \n",
    "    # Calculations for layer 2:\n",
    "    \n",
    "    # compute dL/dw[2]00 \n",
    "    dL_da_2_0 = deriv_loss(a2[0], y)\n",
    "    da_2_0_dz_2_0 = deriv_sigmoid(z2[0])\n",
    "    dz_2_0_dw_2_00 = in2[0]\n",
    "    # multiply\n",
    "    dL_dw_2_00 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_dw_2_00\n",
    "    \n",
    "    # compute dL/dw[2]01  [first two terms are the same as above]\n",
    "    dz_2_0_dw_2_01 = in2[1]\n",
    "    # multiply\n",
    "    dL_dw_2_01 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_dw_2_01\n",
    "    \n",
    "    # compute dL/dw[2]02  [first two terms are the same as above]\n",
    "    dz_2_0_dw_2_02 = in2[2]\n",
    "    # multiply\n",
    "    dL_dw_2_02 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_dw_2_02\n",
    "    \n",
    "    ## Calcs for layer 1:\n",
    "    \n",
    "    # compute dL/dw[1]00  [first two terms are the same as in layer 2]\n",
    "    dz_2_0_da_1_0 = W2[0][1]\n",
    "    da_1_0_dz_1_0 = deriv_sigmoid(z1[0])\n",
    "    dz_1_0_dw_1_00 = x[0]\n",
    "    # multiply\n",
    "    dL_dw_1_00 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_0 * da_1_0_dz_1_0 * dz_1_0_dw_1_00\n",
    "    \n",
    "    # compute dL/dw[1]01  [first four terms are the same as above]\n",
    "    dz_1_0_dw_1_01 = x[1]\n",
    "    # multiply\n",
    "    dL_dw_1_01 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_0 * da_1_0_dz_1_0 * dz_1_0_dw_1_01\n",
    "    \n",
    "    # compute dL/dw[1]10  [first two terms are the same as layer 2]\n",
    "    dz_2_0_da_1_1 = W2[0][2]\n",
    "    da_1_1_dz_1_1 = deriv_sigmoid(z1[1])\n",
    "    dz_1_1_dw_1_10 = x[0]\n",
    "    # multiply\n",
    "    dL_dw_1_10 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_1 * da_1_1_dz_1_1 * dz_1_1_dw_1_10\n",
    "    \n",
    "    # compute dL/dw[1]11  [first four terms are the same as above]\n",
    "    dz_1_1_dw_1_11 = x[1]\n",
    "    # multiply\n",
    "    dL_dw_1_11 = dL_da_2_0 * da_2_0_dz_2_0 * dz_2_0_da_1_1 * da_1_1_dz_1_1 * dz_1_1_dw_1_11\n",
    "    \n",
    "    # return updates to W1 and W2\n",
    "    \n",
    "    delta_W1 = np.array([[dL_dw_1_00, dL_dw_1_01], [dL_dw_1_10, dL_dw_1_11]])\n",
    "    delta_W2 = np.array([[dL_dw_2_00, dL_dw_2_01, dL_dw_2_02]])\n",
    "    return delta_W1, delta_W2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ed825-daf8-42ec-80ed-5276f4ee3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, a1, in2, z2, a2 = forward_prop(W1, W2, X_train[0], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8c9b4-aaf6-41ce-9ebd-ae7f640a140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stochastic gradient descent:\n",
    "\n",
    "W1 = np.array([[.1, .2], [.3, .4]])\n",
    "W2 = np.array([[.5, .6, .7]])\n",
    "ALPHA = .1\n",
    "\n",
    "J_sequence = []\n",
    "\n",
    "print(\"x train is\", X_train)\n",
    "print(\"y train is\", y_train)\n",
    "\n",
    "for ctr in range(0, 10000):\n",
    "    #print(\"Iteration: \", ctr)\n",
    "    #print(\"W =\", W1, W2)\n",
    "    print(\"Cost is\", compute_cost(X_train, y_train, W1, W2))\n",
    "    \n",
    "    for i in range(X_train.shape[0]):   # m\n",
    "        z1, a1, in2, z2, a2 = forward_prop(W1, W2, X_train[i])\n",
    "        delta_W1, delta_W2 = backward_prop(W1, W2, X_train[i], y_train[i], z1, a1, in2, z2, a2)\n",
    "        #print(\"Gradients\", delta_W1, delta_W2)\n",
    "        W1 -= ALPHA * delta_W1\n",
    "        W2 -= ALPHA * delta_W2\n",
    "        #J_sequence.append(compute_cost(X_train, y_train, W1, W2))\n",
    "        \n",
    "    #\"batch\" gradient descent:\n",
    "    #D = [np.full_like(W1, 0),np.full_like(W2, 0)]\n",
    "    #for i in range(X_train.shape[0]):   # m\n",
    "    #    z1, a1, in2, z2, a2 = forward_prop(W1, W2, X_train[i])\n",
    "    #    delta_W1, delta_W2 = backward_prop(W1, W2, X_train[i], y_train[i], z1, a1, in2, z2, a2)\n",
    "    #    #print(\"Gradients\", delta_W1, delta_W2)\n",
    "    #    #W[0][0] -= ALPHA * dL_dw0\n",
    "    #    #W[0][1] -= ALPHA * dL_dw1\n",
    "    #    D[0] += delta_W1\n",
    "    #    D[1] += delta_W2   \n",
    "    #W1 -= ALPHA * D[0]/X_train.shape[0]\n",
    "    #W2 -= ALPHA * D[1]/X_train.shape[0]\n",
    "                              \n",
    "    J_sequence.append(compute_cost(X_train, y_train, W1, W2))\n",
    "    \n",
    "print(\"Final W1, W2:\", W1, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffece826-8fbe-4060-8046-f7d9b41e008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the cost as a function of number of iterations of the\n",
    "# gradient descent algorithm.\n",
    "\n",
    "plt.scatter(range(0, len(J_sequence)), J_sequence)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec83a33-77fd-438e-88cf-e38ad8d57572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate original plot\n",
    "print(W1, W2)\n",
    "\n",
    "positives = np.array([(i, j) for i, j in zip(x_data, y_numeric_noisy) if j >= 2])\n",
    "negatives = np.array([(i, j) for i, j in zip(x_data, y_numeric_noisy) if j < 2])\n",
    "plt.scatter(positives[:,0], positives[:,1])\n",
    "plt.scatter(negatives[:,0], negatives[:,1])\n",
    "\n",
    "y_predicted = [make_prediction([1, x], W1, W2) for x in x_data]\n",
    "positives = np.array([(i, j[0]) for i, j in zip(x_data, y_predicted) if j >= .5])\n",
    "negatives = np.array([(i, j[0]) for i, j in zip(x_data, y_predicted) if j < .5])\n",
    "plt.scatter(positives[:,0], positives[:,1])\n",
    "plt.scatter(negatives[:,0], negatives[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b07d75c-c2c1-4096-b6c8-af8e6d8d8760",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
