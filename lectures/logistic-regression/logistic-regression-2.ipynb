{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc371d0d-9295-46ee-8622-c8aec149fdce",
   "metadata": {},
   "source": [
    "# More Logistic Regression\n",
    "\n",
    "Recap:\n",
    "\n",
    "- Model: $f_w(x) = g(z)$, where\n",
    "\n",
    "    $z = w \\cdot x$, and\n",
    "    \n",
    "    $g(z) = \\dfrac{1}{1+e^{-z}} = \\dfrac{1}{1+e^{-w\\cdot x}}$\n",
    "    \n",
    "- Like linear regression, we will have a training set with $m$ examples $x^{(1)}$ through $x^{(m)}$,\n",
    "and each example $x^{(i)}$ has $n$ different features, $x^{(i)}_1$ through $x^{(i)}_n$.\n",
    "\n",
    "- We also have the \"fake\" feature $x_0$, which is always 1.\n",
    "\n",
    "- We have a vector $\\boldsymbol{w}$ of length $n$, so there is one weight per feature.\n",
    "\n",
    "$$\\boldsymbol{w} = [w_0, w_1, \\ldots, w_n]^T$$\n",
    "\n",
    "- The target, $y$, is always 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e84356b-8431-4dae-b87f-bdf7ea68635c",
   "metadata": {},
   "source": [
    "## Review the cost function for linear regression\n",
    "\n",
    "- For linear regression, our cost function was:\n",
    "\n",
    "$$J(\\boldsymbol{w}) = \\frac{1}{m}\\sum_{i=1}^m \\frac{1}{2} \\left( f_\\boldsymbol{w}(x^{(i)}) - y^{(i)} \\right)^2$$\n",
    "\n",
    "- The only difference is that we put the 1/2 inside the summation, for reasons we'll see soon.\n",
    "\n",
    "- In linear regression, where $f_\\boldsymbol{w}(x) = w \\cdot x$,  this **squared error cost function** always results in a convex curve (a bowl shape) with one global minimum.\n",
    "\n",
    "- In logistic regression, if we change the definition of $f$ to $f(x) =  \\dfrac{1}{1+e^{-w\\cdot x}}$, what will happen is that the cost curve $J(w)$ no longer has a single global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c9b1c2-b854-44cf-b926-f33d9d3cf2cf",
   "metadata": {},
   "source": [
    "- So the squared error cost function is not a good choice for logistic regression.\n",
    "\n",
    "- Instead we will define a different cost function that makes more sense in this context,\n",
    "and will give us a smooth curve for $J$ with a global minimum.  (And therefore we will be able\n",
    "to use gradient descent to find this minimum!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe49e8-bd3a-4b4d-aba3-63b5676986e4",
   "metadata": {},
   "source": [
    "# Redo-ing notation a little bit\n",
    "\n",
    "We're going to give a special name to a specific piece of our cost function $J$ and call it the **loss function**:\n",
    "\n",
    "$$\\text{loss function: } L \\left( f_w(x^{(i)}), y^{(i)} \\right)$$\n",
    "\n",
    "This function is designed to measure the **loss on a single training example**.  That is why it is\n",
    "defined as a function of the output prediction $f_w(x^{(i)})$ (in other words, $\\hat{y}^{(i)}$, and the \"true\" $y$-value, \n",
    "$y^{(i)}$.  We will define\n",
    "this function differently for different machine learning algorithms.\n",
    "\n",
    "In linear regression, we define \n",
    "$$L \\left( f_w(x^{(i)}), y^{(i)} \\right) = \\dfrac{1}{2}\\left( f_\\boldsymbol{w}(x^{(i)}) - y^{(i)} \\right)^2$$\n",
    "\n",
    "In logistic regression, we define\n",
    "\n",
    "$$L \\left( f_w(x^{(i)}), y^{(i)} \\right) = \n",
    "    \\begin{cases}\n",
    "        -\\log\\left( f_w(x^{(i)}) \\right) & \\text{if } y^{(i)}=1 \\\\ \n",
    "        -\\log\\left( 1-f_w(x^{(i)}) \\right) & \\text{if } y^{(i)}=0\n",
    "    \\end{cases}$$\n",
    "    \n",
    "The reasoning behind this is as follows:\n",
    "\n",
    "- If $y=1$, then we use $-\\log\\left( f_w(x^{(i)}) \\right)$ because that expression results in 0 when $f_w(x^{(i)}) = 1$, and very large numbers as \n",
    "$f_w(x^{(i)})$ approaches 0.\n",
    "\n",
    "-  If $y=0$, then we use $-\\log\\left(1- f_w(x^{(i)}) \\right)$ because that expression results in 0 when $f_w(x^{(i)}) = 0$, and very large numbers as \n",
    "$f_w(x^{(i)})$ approaches 1.\n",
    "\n",
    "- Overall, with this loss function, the further the prediction $\\hat{y}=f(x)$ is\n",
    "from the target $y$, the higher the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da129187-1a9a-4329-a399-85e60d922b79",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "So for logistic regression, we will rewrite the cost function $J$ like this:\n",
    "\n",
    "$$J(\\boldsymbol{w}) = \\frac{1}{m}\\sum_{i=1}^m L \\left( \n",
    "    f_\\boldsymbol{w}(x^{(i)}), y^{(i)} \\right)$$\n",
    "    \n",
    "with the loss function $L$ defined as:\n",
    "\n",
    "$$L \\left( f_w(x^{(i)}), y^{(i)} \\right) = \n",
    "    \\begin{cases}\n",
    "        -\\log\\left( f_w(x^{(i)}) \\right) & \\text{if } y^{(i)}=1 \\\\ \n",
    "        -\\log\\left( 1-f_w(x^{(i)}) \\right) & \\text{if } y^{(i)}=0\n",
    "    \\end{cases}$$\n",
    "    \n",
    "(and don't forget, our model $f$ is:)\n",
    "\n",
    "  $$f_{w}(x) = g(w \\cdot x) = \\dfrac{1}{1+e^{-w \\cdot x}}$$\n",
    "    \n",
    "Using our new choice of loss function, the overall cost function $J$ is now\n",
    "convex (just like it is for linear regression), and therefore we can apply\n",
    "gradient descent to it to find the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c588735b-1348-4114-8e88-73fd1e07b522",
   "metadata": {},
   "source": [
    "### Simplifying\n",
    "\n",
    "In fact, there's a way to combine the two pieces of the loss function together:\n",
    "\n",
    "$$L \\left( f_w(x^{(i)}), y^{(i)} \\right) = \n",
    "        -y^{(i)}\\log\\left( f_w(x^{(i)}) \\right)-\n",
    "        (1-y^{(i)})\\log\\left( 1-f_w(x^{(i)}) \\right)$$\n",
    "        \n",
    "Try substituting in $y^{(i)}=0$ or 1 into the equation above and \n",
    "notice how each piece simplifies to either the top or bottom part of\n",
    "the piecewise definition of $L$ from earlier.\n",
    "\n",
    "<HR>\n",
    "    \n",
    "Then, we can rewrite $J$ as:\n",
    "    \n",
    "$$J(\\boldsymbol{w}) = \\frac{1}{m}\\sum_{i=1}^m L \\left( \n",
    "    f_\\boldsymbol{w}(x^{(i)}), y^{(i)} \\right)$$\n",
    "    \n",
    "(substitute in our new definition for $L$, and we get)\n",
    "    \n",
    "$$J(\\boldsymbol{w}) = -\\frac{1}{m}\\sum_{i=1}^m  \\left[ y^{(i)}\\log\\left( f_w(x^{(i)}) \\right)+\n",
    "        (1-y^{(i)})\\log\\left( 1-f_w(x^{(i)}) \\right) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83b443-84ce-4f80-b36e-6169ab475030",
   "metadata": {},
   "source": [
    "### Where does this particular function come from?\n",
    "\n",
    "There are many cost functions we could have chosen; why this one?  It turns\n",
    "out that this function is derived from a statistical principle called\n",
    "maximum likelihood estimation, which you don't need to know about right now, but\n",
    "will give us the convex-ness that we want in a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee99ab6-541f-4132-801d-6a133559105c",
   "metadata": {},
   "source": [
    "## Gradient descent for logistic regression\n",
    "\n",
    "Given:\n",
    "\n",
    "- Model: $$f_{w}(x) = g(w \\cdot x) = \\dfrac{1}{1+e^{-w \\cdot x}}$$\n",
    "\n",
    "- Cost function: $$J(\\boldsymbol{w}) = -\\frac{1}{m}\\sum_{i=1}^m  \\left[ y^{(i)}\\log\\left( f_w(x^{(i)}) \\right)+\n",
    "        (1-y^{(i)})\\log\\left( 1-f_w(x^{(i)}) \\right) \\right]$$\n",
    "        \n",
    "- Goal: Find vector $\\boldsymbol{w}$ that  minimizes $J(w)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7d71a-388a-40fd-afee-88d674383d73",
   "metadata": {},
   "source": [
    "Recall that the general equations for \n",
    "gradient descent are:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\cdot \\dfrac{\\partial}{\\partial w_j} J(\\boldsymbol{w})$$\n",
    "\n",
    "Remember that this equation above is actually a bunch of equations,\n",
    "one for each $w_j$.\n",
    "\n",
    "This might look slightly different than for linear regression, but\n",
    "it's because we no longer have the $b$ parameter; it became $w_0$.\n",
    "\n",
    "### Skipping the math for now...\n",
    "\n",
    "When we take the partial derivative above, we get:\n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial w_j} J(\\boldsymbol{w}) =  \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{w}(x^{(i)}) - y^{(i)} \\right)  x^{(i)}_j$$\n",
    "\n",
    "So the complete update equation for gradient descent becomes:\n",
    "\n",
    "$$w_j = w_j - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{w}(x^{(i)}) - y^{(i)} \\right)  x^{(i)}_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e680e17-cae0-4307-b48e-cfaac7d37830",
   "metadata": {},
   "source": [
    "### That looks weird...\n",
    "\n",
    "This equation is exactly the same update equation we had for linear regression!\n",
    "\n",
    "However, this is just a coincidence: the equations are not actually the same, because the function $f$ is different.\n",
    "\n",
    "Linear regression used $f(x) = w \\cdot x$  (note the dot product)\n",
    "\n",
    "Logistic regression uses $f(x) = \\sigma(w \\cdot x) = \\dfrac{1}{1+e^{-w \\cdot x}}$ (still uses the dot product)\n",
    "\n",
    "However, all the same concepts for logistic regression still apply, like how to pick $\\alpha$,\n",
    "the learning curve, checking for convergence, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e50a5-7486-4f61-b016-de0b82b65e65",
   "metadata": {},
   "source": [
    "## Vectorized version:\n",
    "\n",
    "$$\\boldsymbol{w_\\text{new}} = \\boldsymbol{w_\\text{old}} - \\alpha \\nabla J(\\boldsymbol{w})$$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\boldsymbol{w_\\text{new}} &= \\boldsymbol{w_\\text{old}} - \\alpha \\frac{1}{m}X^T(\\sigma(X\\boldsymbol{w})-\\boldsymbol{y})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765864e-4947-4389-9d2f-f882c76ed69f",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- Feature scaling also applies to logistic regression and will help gradient descent\n",
    "converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26077179-da16-4bed-8e1b-a4c96348e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(v):\n",
    "    t = np.exp(v)\n",
    "    return t/sum(t)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46077910-2748-48d9-96fb-06994dbcfdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "softmax([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88b0f429-7893-4555-bf8f-d72338ceaf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c647679-b15e-4dd3-bbd3-c7f22dabab7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28507647, 0.34346703, 0.37145651])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([1,2,3]))/sum(sigmoid(np.array([1,2,3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3156f59-0828-4d6f-b3f7-1826955d2bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
