{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0ee3593-b78e-4b0e-88ec-2f000df5cbec",
   "metadata": {},
   "source": [
    "# Using regularization to reduce overfitting\n",
    "\n",
    "- Previously, we discussed the concepts of overfitting and underfitting.  Let's review.\n",
    "\n",
    "- **Underfitting** is where our model is not sufficiently powerful enough to capture \n",
    "all of the details of the training data.\n",
    "\n",
    "  - For example, this could be using linear regression with not enough features (for instance, our\n",
    "  data fits a quadratic curve but we don't include quadratic features of our data).\n",
    "\n",
    "- **Overrfitting** is where our model is too powerful, and so captures noise in the \n",
    "training data, or extraneous details of the data. \n",
    "\n",
    "  - For example, this could be using linear regression with too many features (for instance, our\n",
    "  data fits a quadratic curve but we include too many higher-order polynomial features.\n",
    "  \n",
    "  - This can also happen when we include too many features in general: regression (or classification)\n",
    "  can start finding trends in the data that are just coincidental and don't serve as good predictors\n",
    "  in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8828f-c954-4523-a57b-d289fab197cf",
   "metadata": {},
   "source": [
    "## What can we do to address overfitting?\n",
    "\n",
    "Let's say you fit a model to your training set and you have overfitting.  What do you do?\n",
    "\n",
    "- One solution is to collect more training data.  This will tend to smooth out any \n",
    "overfitting in your model, because the model has to take into account more data and can't\n",
    "\"wiggle around\" to fit every training example anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c6266-21f1-456e-ba6c-9c45943aeb20",
   "metadata": {},
   "source": [
    "- This is probably the number-one best strategy to address overfitting.  But sometimes, \n",
    "this isn't an option.  Collecting more data can be expensive (in terms of money, time, or effort),\n",
    "or there just might not be any more.\n",
    "\n",
    "- Another strategy is to reduce the number of features you use.  If you have many features\n",
    "but not a lot of data, overfitting is very likely (this is due to the way mathematics work in\n",
    "high-dimensional spaces, which we will see later).  \n",
    "\n",
    "- Reducing the number of features, (e.g., by using your intuition to focus on only the few you think\n",
    "have the most correlation with the variable you're trying to predict) is called **feature selection**.\n",
    "But it's sometimes hard to do!\n",
    "\n",
    "- Another problem with reducing the number of features is because sometimes you're forced to \n",
    "disregard useful features!  We will see later that there are algorithms to perform this feature\n",
    "selection process for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eed766-c395-4fda-b42d-cc857f406603",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "Another strategy is called **regularization**.  The point of regularization is to reduce the size\n",
    "(magitude)\n",
    "of the parameters $w_j$ in the vector $\\boldsymbol{w}$.\n",
    "\n",
    "We do this because if you look at an overfit model, the parameters are often very large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612d4428-dd3b-49c1-9a39-a9ad969ebea2",
   "metadata": {},
   "source": [
    "If you eliminate a feature, like feature selection does, that's equivalent to \n",
    "setting the equivalent parameter to zero.\n",
    "\n",
    "Regularization is a way of \"softening\" this feature removal process, by not\n",
    "setting a parameter to zero, but rather making sure it doesn't grow too large.  This allows us\n",
    "to have the best of both worlds: we don't remove the feature entirely by setting the\n",
    "parameter to zero, but we don't let  the parameter\n",
    "grow to whatever gradient descent \"thinks\" it should be (which overfits).\n",
    "\n",
    "It turns out that even if you fit a high-order polynomial to a small data set, by limiting the\n",
    "size of the parameters, this will smooth out a lot of the \"wigglyness\" that you would see in an \n",
    "overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b9ea9b-4b00-453e-bc96-4ff8108fa305",
   "metadata": {},
   "source": [
    "So regularization lets you keep all of your features, but prevents them\n",
    "from having overly-large effects on your model.\n",
    "\n",
    "By convention, we usually only regularize $w_j$ for $j \\geq 1$; in other words, we don't regularize\n",
    "$w_0$.  Remember $w_0$ is equivalent to our original $b$ parameter, and can be thought of \n",
    "in algebraic terms as the \"y-intercept\" on an x-y graph, and therefore doesn't usually need to\n",
    "be regularized.  That said, it often doesn't hurt if you regularize it, so some people will do it anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe4361-52d0-4fa1-bdf4-73dc9acfe829",
   "metadata": {},
   "source": [
    "## Summary so far\n",
    "\n",
    "To reduce overfitting:\n",
    "  - Collect more data.\n",
    "  - Select features, drop others.\n",
    "  - Use regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f0720-d6ee-4f48-b297-4bb53954b147",
   "metadata": {},
   "source": [
    "## Adjusting the cost function with regularization\n",
    "\n",
    "Let's examine regularization by returning to linear regression (though the same principles apply to logistic regression).\n",
    "\n",
    "Here's the cost function for linear regression:\n",
    "\n",
    "$$J(w)   = \\frac{1}{2m}\\sum_{i=1}^m \\left( f_w(x^{(i)})-{y}^{(i)} \\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44feb0-7fc9-41c4-9afb-1b184d76388e",
   "metadata": {},
   "source": [
    "What regularization does is add to the cost function the raw weights $w_j$ themselves,\n",
    "weighted by a (often large) constant.  Because we are trying to minimize the cost function,\n",
    "this tends to keep the weights small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfcf68-6adf-436c-b3f4-918d98bed0a9",
   "metadata": {},
   "source": [
    "This penalizes large weights, and tends to cause gradient descent to want to keep\n",
    "those weights small.\n",
    "\n",
    "This tends to reduce overfitting.\n",
    "\n",
    "This also works the same way if you have lots of unrelated features (rather than a series of\n",
    "higher-order features all derived from the same piece of data).  If we penalize *all* the weights\n",
    "for getting large, this will tend to keep them small, and make it less likely for our model \n",
    "to overfit.\n",
    "\n",
    "We then don't have to worry about picking which features to include or exclude --- just regularize\n",
    "everything and let gradient descent handle it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f617f52-6299-40fb-b496-912517b210e8",
   "metadata": {},
   "source": [
    "### New cost function\n",
    "\n",
    "$$J(w)  = \\frac{1}{2m}\\sum_{i=1}^m \\left( f_w(x^{(i)})-{y}^{(i)} \\right)^2 + \\dfrac{\\lambda}{2m}\n",
    "\\sum_{j=1}^n w_j^2$$\n",
    "\n",
    "Note the new term at the end.\n",
    "\n",
    "The $\\lambda$ (lambda) is called the \"regularization parameter.\"  Like $\\alpha$, it's another constant\n",
    "we have to pick a value for.  \n",
    "\n",
    "We also then divide $\\lambda$ by $2m$, which is a little silly, because we could just build that into\n",
    "the $\\lambda$ constant itself, but it makes the derivative nicer later.  It also makes it easier to pick\n",
    "a good $\\lambda$ in the first place if we scale both pieces of the cost equation in the same fashion.\n",
    "The reason for this is as the size of the data set increases (and $m$ grows larger), we often need\n",
    "to scale down the regularization parameter.\n",
    "\n",
    "Also note that the summation for the regularization term doesn't include $w_0$.\n",
    "\n",
    "The two terms (components) of the cost equation have names: we call the left part the **mean squared\n",
    "error** and the right part is the **regularization term**.\n",
    "\n",
    "These two terms have somewhat opposing goals when we minimize $J$:\n",
    "  - Minimizing the mean squared error encourages gradient descent to find a model that fits the \n",
    "  data well.\n",
    "  \n",
    "  - Minimizing the regularization term encourages gradient to descent to find a model that keeps\n",
    "  the $w_j$'s small (and therefore reduces overfitting).\n",
    "  \n",
    "The $\\lambda$ parameter controls the balance between these two opposing goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db41326f-88a4-4c8b-8ca0-e69de66b87e8",
   "metadata": {},
   "source": [
    "As an example, if $\\lambda = 0$ (or is too small), the model will overfit.  If \n",
    "$\\lambda$ is too big, the model will underfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8915af64-33de-4746-af15-7b0d18946120",
   "metadata": {},
   "source": [
    "## Math time!\n",
    "\n",
    "## Regularized linear regression\n",
    "\n",
    "When we add regularization to our cost function, nothing changes about the overall\n",
    "method of approaching linear regression: we're still going to use gradient descent,\n",
    "and so we still have to minimize our cost function $J$.\n",
    "\n",
    "Find the parameter vector $w$ to minimize:\n",
    "\n",
    "$$J(w)  = \\frac{1}{2m}\\sum_{i=1}^m \\left( f_w(x^{(i)})-{y}^{(i)} \\right)^2 + \\dfrac{\\lambda}{2m}\n",
    "\\sum_{j=1}^n w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfdf65-fc77-4b57-bdce-78af5fca40cd",
   "metadata": {},
   "source": [
    "Our new update equation for gradient descent:\n",
    "    \n",
    "$$w_j = w_j - \\alpha  \\left[ \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w}}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x_j^{(i)} + \\dfrac{\\lambda}{m}w_j \\right] \\qquad \\text{for $j>0$}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144531d0-ab2d-4504-aea6-7e493b47fe9b",
   "metadata": {},
   "source": [
    "(for $w_0$ the equation remains the same as before, because we don't regularize $w_0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697ee42-1f8f-41f9-93e4-f0316bb3c6fc",
   "metadata": {},
   "source": [
    "### A little mathematical explanation\n",
    "\n",
    "Another way to write the $w_j$ update equation is:\n",
    "    \n",
    "$$w_j = w_j - \\alpha\\dfrac{\\lambda}{m}w_j -  \\alpha\\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w}}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x_j^{(i)}  $$\n",
    "\n",
    "which can be rewritten as\n",
    "\n",
    "$$w_j = w_j\\left(1 - \\alpha\\dfrac{\\lambda}{m}\\right) -  \\alpha\\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w}}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x_j^{(i)}  $$\n",
    "\n",
    "It is interesting to think about that $\\left(1 - \\alpha\\dfrac{\\lambda}{m}\\right)$ term --- it\n",
    "will usually be a decimal number slightly smaller than one.  Multiplying a number like that\n",
    "(e.g., 0.999) has the effect of slightly shrinking $w_j$ on each update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb4151-6d74-4d57-ac6f-e5d8d73f7411",
   "metadata": {},
   "source": [
    "## Regularized logistic regression\n",
    "\n",
    "We apply the same regularization term to our cost function $J$ for logistic regression:\n",
    "\n",
    "$$J(\\boldsymbol{w}) = -\\frac{1}{m}\\sum_{i=1}^m  \\left[ y^{(i)}\\log\\left( f_w(x^{(i)}) \\right)+\n",
    "        (1-y^{(i)})\\log\\left( 1-f_w(x^{(i)}) \\right) \\right] + \\dfrac{\\lambda}{2m}\n",
    "\\sum_{j=1}^n w_j^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc3cf0-2b6c-4c97-a948-34960967482f",
   "metadata": {},
   "source": [
    "And similarly to linear regression, our new update equation changes to:\n",
    "    \n",
    "$$w_j = w_j - \\alpha  \\left[ \\frac{1}{m} \\sum_{i=1}^m  \\left( f_{\\boldsymbol{w}}(\\boldsymbol{x}^{(i)}) - y^{(i)} \\right)  x_j^{(i)} + \\dfrac{\\lambda}{m}w_j \\right] \\qquad \\text{for $j>0$}$$\n",
    "\n",
    "Just recall that $f$ here is different than $f$ for linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
